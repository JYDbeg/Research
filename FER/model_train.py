# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WCGO9GYQJAFCc2mDeKvoXoFb1Kosw1en
"""

!tar -xvf "/content/drive/MyDrive/train_set.tar" -C "/content/train_data/"

!tar -xvf "/content/drive/MyDrive/val_set.tar" -C "/content/train_data/"

#!unrar x "/content/drive/MyDrive/val_class.rar" "/content/train_data/val_class/"
#!unrar x "/content/drive/MyDrive/train_class.rar" "/content/train_data/train_class/"

!pip install timm
!pip install mediapipe
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from drive.MyDrive.poolformer import poolformer_s12
from PIL import Image
from tqdm import tqdm
import cv2
import mediapipe as mp

mp_face_detection = mp.solutions.face_detection
face_detection =mp_face_detection.FaceDetection(
    model_selection=0, min_detection_confidence=0.3)
class pathdataset(torch.utils.data.Dataset):
    def __init__(self,x,y,z):
        super().__init__()
        self.x =x
        self.y =y
        self.z =z
    def __len__(self):
        return len(self.x)
    def __getitem__(self,idx):
        img =cv2.imread(self.x[idx])
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        po = []
        name=[]
        pos = []
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        result = face_detection.process(img)
        if result.detections:
            for detection in result.detections:
                data = detection.location_data
                for i in range(len(mp_face_detection.FaceKeyPoint)):
                    po.append(data.relative_keypoints[mp_face_detection.FaceKeyPoint(i).value])
                    name.append(mp_face_detection.FaceKeyPoint(i).name)
            for i in po[:4]:
                pos.append([int(i.x*224),int(i.y*224)])
        weight = np.zeros((224,224,3))
        weight = weight+0.3
        for i in pos:
            if pos.index(i)<2:
                weight[i[1]-50:i[1]+10,i[0]-40:i[0]+40] = 1
            elif pos.index(i) == 2:
                weight[i[1]-80:i[1]+25,i[0]-35:i[0]+35] =1
            else:
                weight[i[1]-15:i[1]+30,i[0]-35:i[0]+45] =1
        img = img*weight
        img = np.array(img)
        img = img/255.0
        img = img.transpose(2,0,1)
        v = np.array(np.load(self.y[idx],allow_pickle=1))
        a = np.array(np.load(self.z[idx],allow_pickle=1))
        v = v.astype(np.float32)
        v = (v+1)/2
        a = a.astype(np.float32)
        a = (a+1)/2
        va = np.array([v,a])
        feature = torch.FloatTensor(img)
        label = torch.FloatTensor(va)
        return feature,label
class Traindataset(torch.utils.data.Dataset):
    def __init__(self,x,y):
        self.x =x
        self.y =y
    def __len__(self):
        return len(self.x)
    def __getitem__(self,idx):
        img =cv2.imread(self.x[idx])
        po = []
        name=[]
        pos = []
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        result = face_detection.process(img)
        if result.detections:
            for detection in result.detections:
                data = detection.location_data
                for i in range(len(mp_face_detection.FaceKeyPoint)):
                    po.append(data.relative_keypoints[mp_face_detection.FaceKeyPoint(i).value])
                    name.append(mp_face_detection.FaceKeyPoint(i).name)
            for i in po[:4]:
                pos.append([int(i.x*224),int(i.y*224)])
        weight = np.zeros((224,224,3))
        weight = weight+0.5
        for i in pos:
            if pos.index(i)<2:
                weight[i[1]-50:i[1]+10,i[0]-40:i[0]+40] = 1
            elif pos.index(i) == 2:
                weight[i[1]-80:i[1]+25,i[0]-35:i[0]+35] =1
            else:
                weight[i[1]-15:i[1]+30,i[0]-35:i[0]+45] =1
        img = img*weight
        img = np.array(img)
        img = img/255.0
        img = img.transpose(2,0,1)
        l = np.array(np.load(self.y[idx],allow_pickle=1))
        feature = torch.FloatTensor(img)
        label = torch.tensor(l,dtype=torch.long)
        return feature,label

from pathlib import Path
import glob
p_1 = "/content/train_data/train_set/images/*.jpg"
p_2 = "/content/train_data/train_set/annotations/"
p_3 = "/content/train_data/val_set/images/*.jpg"
p_4 = "/content/train_data/val_set/annotations/"
paths = []
v_paths = []
a_paths = []
f_c = []
n_c = []
o_c =[]
t_c = []
for file in tqdm(glob.glob(p_1)):

    file_name = Path(file).stem
    #data = np.array(Image.open(file))
    v = np.load(p_2+file_name+"_val.npy",allow_pickle =1)
    a = np.load(p_2+file_name+"_aro.npy",allow_pickle =1)
    
    if v == -2 or a==-2:
        continue
    #va = np.array([v,a])
    paths.append(f"/content/train_data/train_set/images/{file_name}.jpg")
    v_paths.append(f"/content/train_data/train_set/annotations/{file_name}_val.npy")
    a_paths.append(f"/content/train_data/train_set/annotations/{file_name}_aro.npy")
    '''f_c.append(f"/content/train_data/train_class/class/{file_name}_f.npy")
    n_c.append(f"/content/train_data/train_class/class/{file_name}_n.npy")
    o_c.append(f"/content/train_data/train_class/class/{file_name}_o.npy")
    t_c.append(f"/content/train_data/train_class/class/{file_name}_t.npy")'''
    #save_data = {file_name:{"img":data,"va":va}}
    #save_datas.update(**save_data)
#np.savez_compressed("E:/affecttraindata",**save_datas)
#sava_datas = {}
#test_data = {}
vpaths = []
vv_paths = []
va_paths = []
vf_c = []
vn_c = []
vo_c =[]
vt_c = []
for file in tqdm(glob.glob(p_3)):
    file_name = Path(file).stem
    #data = np.array(Image.open(file))
    v = np.load(p_4+file_name+"_val.npy",allow_pickle =1)
    a = np.load(p_4+file_name+"_aro.npy",allow_pickle =1)
    if v == -2 or a==-2:
        continue
    vpaths.append(f"/content/train_data/val_set/images/{file_name}.jpg")
    vv_paths.append(f"/content/train_data/val_set/annotations/{file_name}_val.npy")
    va_paths.append(f"/content/train_data/val_set/annotations/{file_name}_aro.npy")
    '''vf_c.append(f"/content/train_data/val_class/class/{file_name}_f.npy")
    vn_c.append(f"/content/train_data/val_class/class/{file_name}_n.npy")
    vo_c.append(f"/content/train_data/val_class/class/{file_name}_o.npy")
    vt_c.append(f"/content/train_data/val_class/class/{file_name}_t.npy")'''

traindataset =pathdataset(paths,v_paths,a_paths)
#traindataset =Traindataset(paths,t_c)

valdataset = pathdataset(vpaths,vv_paths,va_paths)
#valdataset = Traindataset(vpaths,vt_c)
dataloader = torch.utils.data.DataLoader(traindataset,batch_size= 128 ,shuffle =True,num_workers=0)
v_dataloader = torch.utils.data.DataLoader(valdataset,batch_size = 128,shuffle = False,num_workers=0)

model = poolformer_s12()
isTrained = 0
if isTrained:
  check_point = torch.load("/content/drive/MyDrive/myOneHundredClassModel.pth")
  for param in model.parameters():
    param.requires_grad = False
  last_layer = list(model.children())[-3:-1]
  for layer in last_layer:
      for param in layer.parameters():
          param.requires_grad = True
  model.head = nn.Sequential(nn.Linear(512,250),nn.ReLU(),nn.Linear(250,12))
  model.load_state_dict(check_point)
else:
  check_point =torch.load("/content/drive/MyDrive/poolformer_s12.pth.tar")
  model.load_state_dict(check_point)
  for param in model.parameters():
    param.requires_grad = False
  last_layer = list(model.children())[-3:-1]
  for layer in last_layer:
      for param in layer.parameters():
          param.requires_grad = True
  model.head = nn.Sequential(nn.Linear(512,250),nn.ReLU(),nn.Linear(250,2),nn.Identity())#,nn.Tanh()
#weight = torch.tensor([287651/27792,287651/109927,287651/6542, 287651/16033,287651/50027,287651/7445,287651/35104,287651/15774,287651/19007]).cuda()
class PercentLoss(nn.Module):
    def __init__(self): # パラメータの設定など初期化処理を行う
        super(PercentLoss, self).__init__()

    def forward(self, outputs, targets): # モデルの出力と正解データ

        loss = torch.sum(torch.abs(targets-outputs)/targets)
        return loss
loss = PercentLoss()#nn.MSELoss()
optimizer = optim.Adam(model.parameters(),lr = 0.0001)
device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
model.to(device)

def saveModel():
    path = "/content/drive/MyDrive/myFirstModelMixedPlusCustomLoss.pth"
    torch.save(model.state_dict(), path)

min_val_loss = 1e9
es_epoch = 20
count = 0
for i in range(100):

  print(dataloader.dataset[i][1])
for epoch in range(10000):
    if epoch >=10:
      saveModel()
      break
    running_loss = 0.0
    val_loss = 0.0
    model.train()
    for i,data in tqdm(enumerate(dataloader)):
        inputs,labels = data[0].to(device), data[1].to(device)
        optimizer.zero_grad()
        outputs = model(inputs)
        train_loss = torch.sqrt(loss(outputs,labels))*100#loss(outputs,labels)#torch.sqrt(loss(outputs,labels))
        train_loss.backward()
        optimizer.step()
        running_loss+=train_loss.item()
    avg_train_loss = running_loss/len(dataloader.dataset)
    model.eval()
    with torch.no_grad():
        for data,labels in v_dataloader:
            data = data.to(device)
            labels = labels.to(device)
            outputs = model(data)
            l = torch.sqrt(loss(outputs,labels))*100#loss(outputs,labels)#torch.sqrt(loss(outputs,labels))
            val_loss+=l.item()
        avg_val_loss = val_loss/len(v_dataloader.dataset)
        if avg_val_loss>min_val_loss:
            es_epoch-=1
        else:
            min_val_loss = min(avg_val_loss,min_val_loss)
            saveModel()
        if not es_epoch:
            saveModel()
            break
        
        print ('Epoch [{}/{}], loss: {loss:.4f} val_loss: {val_loss:.4f},' 
                .format(epoch+1, 10000, i+1, loss=avg_train_loss, val_loss=avg_val_loss))

!nvidia-smi